{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch之nn模块练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#库\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义网络\n",
    "class Linear(nn.Module): #继承nn.Module\n",
    "\n",
    "    def __init__(self, in_features, out_features): #输入数据的维度、输出数据的维度\n",
    "        super(Linear,self).__init__()\n",
    "        \n",
    "        self.w=nn.Parameter(torch.randn(in_features,out_features))#可学习的权重矩阵——标准正态分布初始化\n",
    "        self.b=nn.Parameter(torch.randn(out_features))#可学习的偏置向量——标准正统分布初始化\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.mm(self.w) #乘\n",
    "        return x+self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3185, -1.9280, -0.2926],\n",
       "        [-0.2435, -0.2010, -2.2742],\n",
       "        [-1.9942, -4.9260,  3.2610],\n",
       "        [-1.4837, -2.4830,  2.5660]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#网络正向传播\n",
    "layer=Linear(5,3)\n",
    "input=torch.randn(4,5)\n",
    "output=layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-0.2819, -1.6520,  0.4598],\n",
      "        [ 0.8439,  0.2167, -0.8900],\n",
      "        [-0.8746, -0.6107,  0.8331],\n",
      "        [-0.2925, -0.6853, -0.5600],\n",
      "        [-0.4001,  0.0921, -0.6926]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.5442, -1.1021, -0.2497], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#查看参数\n",
    "for name, parameter in layer.named_parameters():\n",
    "    print(name, parameter) #即w，b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.w torch.Size([3, 5])\n",
      "layer1.b torch.Size([5])\n",
      "layer2.w torch.Size([5, 1])\n",
      "layer2.b torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "#使用sigmoid函数作为激发函数的网络\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features,out_features):\n",
    "        super(Perceptron,self).__init__()\n",
    "\n",
    "        self.layer1=Linear(in_features,hidden_features)\n",
    "        self.layer2=Linear(hidden_features,out_features)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=torch.sigmoid(x)\n",
    "        return self.layer2(x)\n",
    "    \n",
    "perceptron=Perceptron(3,5,1)\n",
    "for name,param in perceptron.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1: Sequential(\n",
      "  (Linear1): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (Linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (activation_layer): Sigmoid()\n",
      ")\n",
      "net2: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (2): Sigmoid()\n",
      ")\n",
      "net3: Sequential(\n",
      "  (Linear1): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (Linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (activation_layer): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#第一种\n",
    "net1 = nn.Sequential()\n",
    "net1.add_module('Linear1', nn.Linear(4, 3))\n",
    "net1.add_module('Linear2', nn.Linear(3, 1))\n",
    "net1.add_module('activation_layer', nn.Sigmoid())\n",
    "\n",
    "#第二种\n",
    "net2 = nn.Sequential(\n",
    "    nn.Linear(4, 3),\n",
    "    nn.Linear(3, 1),\n",
    "    nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "#第三种\n",
    "from collections import OrderedDict\n",
    "net3 = nn.Sequential(OrderedDict([\n",
    "    ('Linear1', nn.Linear(4, 3)),\n",
    "    ('Linear2', nn.Linear(3, 1)),\n",
    "    ('activation_layer',nn.Sigmoid())\n",
    "    ]))\n",
    "print('net1:', net1)\n",
    "print('net2:', net2)\n",
    "print('net3:', net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=4, out_features=3, bias=True),\n",
       " Linear(in_features=4, out_features=3, bias=True),\n",
       " Linear(in_features=4, out_features=3, bias=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可根据名字后序号取出子module\n",
    "net1.Linear1, net2[0], net3.Linear1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3482],\n",
       "         [0.3716],\n",
       "         [0.3339],\n",
       "         [0.3181]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.4510],\n",
       "         [0.4628],\n",
       "         [0.4652],\n",
       "         [0.5064]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.4314],\n",
       "         [0.4745],\n",
       "         [0.4169],\n",
       "         [0.4029]], grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.3482],\n",
       "         [0.3716],\n",
       "         [0.3339],\n",
       "         [0.3181]], grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(4,4)\n",
    "output1 = net1(input)\n",
    "output2 = net2(input)\n",
    "output3 = net3(input)\n",
    "output4 = net3.activation_layer(net1.Linear2(net1.Linear1(input)))\n",
    "output1,output2,output3,output4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModule(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=3, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.list=[nn.Linear(3,4),nn.Sigmoid()]\n",
    "        self.module_list=nn.ModuleList([nn.Linear(4,3),nn.Sigmoid()])\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "model=MyModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_list.0.weight torch.Size([3, 4])\n",
      "module_list.0.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3,3)\n",
    "model = nn.Linear(3,4)\n",
    "output1 = model(input)\n",
    "#使用上面使用的w,b，两种写法返回的结果是相同的\n",
    "output2 = nn.functional.linear(input, model.weight, model.bias)\n",
    "output1 == output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = nn.functional.sigmoid(input)\n",
    "b2 = nn.Sigmoid()(input)\n",
    "b1 == b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4894,  0.4143, -0.0041],\n",
      "        [ 0.1547, -0.4861,  0.7648]])\n",
      "tensor([[0.4894, 0.4143, 0.0000],\n",
      "        [0.1547, 0.0000, 0.7648]])\n"
     ]
    }
   ],
   "source": [
    "relu=nn.ReLU(inplace=True)\n",
    "input=torch.randn(2,3)\n",
    "print(input)\n",
    "output=relu(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "pred = torch.tensor([1.,2.,3.,4.,5.])\n",
    "target =torch.tensor([1.,1.,1.,1.,1.])\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(pred, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3788)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross entropy loss\n",
    "# batch_size=3,计算对应的每个类别的分数(只有两个类别)\n",
    "pred = torch.randn(3,2)\n",
    "# 三个样本分别属于1,0,1类，\n",
    "target=torch.tensor([1,1,1])\n",
    "CELoss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss2 = CELoss(pred, target)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.2498, -0.1959, -0.6250],\n",
       "        [-0.5790, -0.3113, -0.9196],\n",
       "        [-0.6987,  0.5840, -0.0751],\n",
       "        [ 0.4066, -0.1735, -0.3879]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#利用nn.init初始化\n",
    "from torch.nn import init\n",
    "linear = nn.Linear(3,4)\n",
    "\n",
    "#等价于linear.weight.data.normal_(0, std),std是正态分布的标准差\n",
    "init.xavier_normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2881, -0.0278, -0.2095,  0.0700],\n",
      "        [ 0.3854,  0.1827,  0.8569,  0.0608],\n",
      "        [-0.3087,  0.0730,  0.1381,  0.1736]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1000, 0.1000, 0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net1 = nn.Sequential()\n",
    "net1.add_module('Linear1', nn.Linear(4, 3))\n",
    "net1.add_module('Linear2', nn.Linear(3, 1))\n",
    "net1.add_module('activation_layer', nn.Sigmoid())\n",
    "\n",
    "\n",
    "print(init.xavier_uniform_(net1.Linear1.weight))\n",
    "print(init.constant_(net1.Linear1.bias, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Linear1): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (Linear2): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (activation_layer): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname=m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        init.xavier_uniform_(m.weight.data)\n",
    "        init.constant_(m.bias.data,0.1)\n",
    "\n",
    "net1.apply(weights_init) #apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。  \n",
    "            #对所有的Conv层都初始化权重. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课堂作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=784, out_features=224, bias=True)\n",
      "  (fc2): Linear(in_features=224, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # 隐藏层\n",
    "        self.fc1 = nn.Linear(784, 224)  # 输入层到第一隐藏层\n",
    "        # 第一隐藏层到第二隐藏层\n",
    "        self.fc2 = nn.Linear(224, 128) \n",
    "        # 第二隐藏层到第三隐藏层\n",
    "        self.fc3 = nn.Linear(128, 64)       \n",
    "        # 输出层\n",
    "        self.fc4 = nn.Linear(64, 10)     # 第三隐藏层到输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)   # 将输入张量展平成一维\n",
    "        \n",
    "        x = F.relu(self.fc1(x))  # 输入层到第一隐藏层的前向传播，使用ReLU作为激活函数\n",
    "        # 第一隐藏层到第二隐藏层的前向传播，使用ReLU作为激活函数\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 第二隐藏层到第三隐藏层的前向传播，使用ReLU作为激活函数\n",
    "        x = F.relu(self.fc3(x))    \n",
    "        x = self.fc4(x)  # 第三隐藏层到输出层的前向传播\n",
    "        return F.log_softmax(x, dim=1)  # 使用log_softmax作为输出的激活函数\n",
    "\n",
    "# 创建网络实例\n",
    "net = SimpleNet()\n",
    "# 打印网络\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
