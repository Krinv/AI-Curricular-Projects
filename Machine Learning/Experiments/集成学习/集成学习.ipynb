{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 10), (3000, 10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "data, target = make_hastie_10_2()\n",
    "\n",
    "# 将-1替换为0\n",
    "target[target == -1] = 0\n",
    "# 确保数据类型为整数\n",
    "target = target.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=123)\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54766667 (+/- 0.01),耗时0.04秒。模型名称[Logistic Regression]\n",
      "Accuracy: 0.88211111 (+/- 0.01),耗时43.21秒。模型名称[Random Forest]\n",
      "Accuracy: 0.87722222 (+/- 0.01),耗时7.22秒。模型名称[AdaBoost]\n",
      "Accuracy: 0.91122222 (+/- 0.01),耗时34.88秒。模型名称[GBDT]\n",
      "Accuracy: 0.92033333 (+/- 0.01),耗时2.39秒。模型名称[XGBoost]\n",
      "[LightGBM] [Info] Number of positive: 3514, number of negative: 3686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488056 -> initscore=-0.047787\n",
      "[LightGBM] [Info] Start training from score -0.047787\n",
      "[LightGBM] [Info] Number of positive: 3514, number of negative: 3686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488056 -> initscore=-0.047787\n",
      "[LightGBM] [Info] Start training from score -0.047787\n",
      "[LightGBM] [Info] Number of positive: 3514, number of negative: 3686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488056 -> initscore=-0.047787\n",
      "[LightGBM] [Info] Start training from score -0.047787\n",
      "[LightGBM] [Info] Number of positive: 3515, number of negative: 3685\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488194 -> initscore=-0.047231\n",
      "[LightGBM] [Info] Start training from score -0.047231\n",
      "[LightGBM] [Info] Number of positive: 3515, number of negative: 3685\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 7200, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488194 -> initscore=-0.047231\n",
      "[LightGBM] [Info] Start training from score -0.047231\n",
      "Accuracy: 0.92544444 (+/- 0.01),耗时1.25秒。模型名称[LightGBM]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = GradientBoostingClassifier()\n",
    "clf5 = XGBClassifier()\n",
    "clf6 = LGBMClassifier()\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], [\n",
    "        'Logistic Regression', 'Random Forest', 'AdaBoost', 'GBDT', 'XGBoost',\n",
    "        'LightGBM'\n",
    "]):\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    end = time.time()\n",
    "    running_time = end - start\n",
    "    print(\"Accuracy: %0.8f (+/- %0.2f),耗时%0.2f秒。模型名称[%s]\" %\n",
    "          (scores.mean(), scores.std(), running_time, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#记录程序运行时间\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#xgb矩阵赋值\n",
    "xgb_train = xgb.DMatrix(X_train, y_train)\n",
    "xgb_test = xgb.DMatrix(X_test, label=y_test)\n",
    "##参数\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "#     'silent': 1,  #设置成1则没有运行信息输出，最好是设置为0.\n",
    "    #'nthread':7,# cpu 线程数 默认最大\n",
    "    'eta': 0.007,  # 如同学习率\n",
    "    'min_child_weight': 3,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    'max_depth': 6,  # 构建树的深度，越大越容易过拟合\n",
    "    'gamma': 0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    'subsample': 0.7,  # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,  # 生成树时进行的列采样 \n",
    "    'lambda': 2,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #'alpha':0, # L1 正则项参数\n",
    "    #'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。\n",
    "    #'objective': 'multi:softmax', #多分类的问题\n",
    "    #'num_class':10, # 类别数，多分类与 multisoftmax 并用\n",
    "    'seed': 1000,  #随机种子\n",
    "    #'eval_metric': 'auc'\n",
    "}\n",
    "plst = list(params.items())\n",
    "num_rounds = 500  # 迭代次数\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.49922\tval-rmse:0.49973\n",
      "[1]\ttrain-rmse:0.49857\tval-rmse:0.49918\n",
      "[2]\ttrain-rmse:0.49791\tval-rmse:0.49862\n",
      "[3]\ttrain-rmse:0.49731\tval-rmse:0.49814\n",
      "[4]\ttrain-rmse:0.49671\tval-rmse:0.49761\n",
      "[5]\ttrain-rmse:0.49607\tval-rmse:0.49707\n",
      "[6]\ttrain-rmse:0.49545\tval-rmse:0.49652\n",
      "[7]\ttrain-rmse:0.49484\tval-rmse:0.49596\n",
      "[8]\ttrain-rmse:0.49424\tval-rmse:0.49546\n",
      "[9]\ttrain-rmse:0.49360\tval-rmse:0.49489\n",
      "[10]\ttrain-rmse:0.49297\tval-rmse:0.49435\n",
      "[11]\ttrain-rmse:0.49237\tval-rmse:0.49382\n",
      "[12]\ttrain-rmse:0.49172\tval-rmse:0.49321\n",
      "[13]\ttrain-rmse:0.49109\tval-rmse:0.49267\n",
      "[14]\ttrain-rmse:0.49049\tval-rmse:0.49212\n",
      "[15]\ttrain-rmse:0.48989\tval-rmse:0.49159\n",
      "[16]\ttrain-rmse:0.48927\tval-rmse:0.49106\n",
      "[17]\ttrain-rmse:0.48864\tval-rmse:0.49050\n",
      "[18]\ttrain-rmse:0.48803\tval-rmse:0.48997\n",
      "[19]\ttrain-rmse:0.48744\tval-rmse:0.48949\n",
      "[20]\ttrain-rmse:0.48685\tval-rmse:0.48895\n",
      "[21]\ttrain-rmse:0.48628\tval-rmse:0.48842\n",
      "[22]\ttrain-rmse:0.48569\tval-rmse:0.48795\n",
      "[23]\ttrain-rmse:0.48511\tval-rmse:0.48740\n",
      "[24]\ttrain-rmse:0.48451\tval-rmse:0.48686\n",
      "[25]\ttrain-rmse:0.48395\tval-rmse:0.48635\n",
      "[26]\ttrain-rmse:0.48334\tval-rmse:0.48582\n",
      "[27]\ttrain-rmse:0.48275\tval-rmse:0.48526\n",
      "[28]\ttrain-rmse:0.48218\tval-rmse:0.48474\n",
      "[29]\ttrain-rmse:0.48161\tval-rmse:0.48423\n",
      "[30]\ttrain-rmse:0.48105\tval-rmse:0.48374\n",
      "[31]\ttrain-rmse:0.48050\tval-rmse:0.48322\n",
      "[32]\ttrain-rmse:0.47991\tval-rmse:0.48271\n",
      "[33]\ttrain-rmse:0.47930\tval-rmse:0.48221\n",
      "[34]\ttrain-rmse:0.47870\tval-rmse:0.48172\n",
      "[35]\ttrain-rmse:0.47811\tval-rmse:0.48119\n",
      "[36]\ttrain-rmse:0.47755\tval-rmse:0.48068\n",
      "[37]\ttrain-rmse:0.47700\tval-rmse:0.48017\n",
      "[38]\ttrain-rmse:0.47646\tval-rmse:0.47969\n",
      "[39]\ttrain-rmse:0.47588\tval-rmse:0.47924\n",
      "[40]\ttrain-rmse:0.47531\tval-rmse:0.47877\n",
      "[41]\ttrain-rmse:0.47474\tval-rmse:0.47828\n",
      "[42]\ttrain-rmse:0.47420\tval-rmse:0.47782\n",
      "[43]\ttrain-rmse:0.47366\tval-rmse:0.47737\n",
      "[44]\ttrain-rmse:0.47310\tval-rmse:0.47685\n",
      "[45]\ttrain-rmse:0.47255\tval-rmse:0.47639\n",
      "[46]\ttrain-rmse:0.47199\tval-rmse:0.47588\n",
      "[47]\ttrain-rmse:0.47145\tval-rmse:0.47540\n",
      "[48]\ttrain-rmse:0.47089\tval-rmse:0.47494\n",
      "[49]\ttrain-rmse:0.47032\tval-rmse:0.47443\n",
      "[50]\ttrain-rmse:0.46978\tval-rmse:0.47399\n",
      "[51]\ttrain-rmse:0.46927\tval-rmse:0.47350\n",
      "[52]\ttrain-rmse:0.46873\tval-rmse:0.47304\n",
      "[53]\ttrain-rmse:0.46820\tval-rmse:0.47254\n",
      "[54]\ttrain-rmse:0.46764\tval-rmse:0.47209\n",
      "[55]\ttrain-rmse:0.46711\tval-rmse:0.47164\n",
      "[56]\ttrain-rmse:0.46655\tval-rmse:0.47113\n",
      "[57]\ttrain-rmse:0.46601\tval-rmse:0.47059\n",
      "[58]\ttrain-rmse:0.46545\tval-rmse:0.47011\n",
      "[59]\ttrain-rmse:0.46492\tval-rmse:0.46964\n",
      "[60]\ttrain-rmse:0.46438\tval-rmse:0.46916\n",
      "[61]\ttrain-rmse:0.46385\tval-rmse:0.46869\n",
      "[62]\ttrain-rmse:0.46332\tval-rmse:0.46826\n",
      "[63]\ttrain-rmse:0.46278\tval-rmse:0.46778\n",
      "[64]\ttrain-rmse:0.46223\tval-rmse:0.46725\n",
      "[65]\ttrain-rmse:0.46169\tval-rmse:0.46681\n",
      "[66]\ttrain-rmse:0.46116\tval-rmse:0.46633\n",
      "[67]\ttrain-rmse:0.46062\tval-rmse:0.46586\n",
      "[68]\ttrain-rmse:0.46010\tval-rmse:0.46541\n",
      "[69]\ttrain-rmse:0.45960\tval-rmse:0.46499\n",
      "[70]\ttrain-rmse:0.45906\tval-rmse:0.46451\n",
      "[71]\ttrain-rmse:0.45857\tval-rmse:0.46405\n",
      "[72]\ttrain-rmse:0.45803\tval-rmse:0.46359\n",
      "[73]\ttrain-rmse:0.45751\tval-rmse:0.46315\n",
      "[74]\ttrain-rmse:0.45697\tval-rmse:0.46268\n",
      "[75]\ttrain-rmse:0.45645\tval-rmse:0.46221\n",
      "[76]\ttrain-rmse:0.45591\tval-rmse:0.46173\n",
      "[77]\ttrain-rmse:0.45538\tval-rmse:0.46126\n",
      "[78]\ttrain-rmse:0.45486\tval-rmse:0.46079\n",
      "[79]\ttrain-rmse:0.45434\tval-rmse:0.46036\n",
      "[80]\ttrain-rmse:0.45385\tval-rmse:0.45997\n",
      "[81]\ttrain-rmse:0.45334\tval-rmse:0.45951\n",
      "[82]\ttrain-rmse:0.45282\tval-rmse:0.45905\n",
      "[83]\ttrain-rmse:0.45231\tval-rmse:0.45859\n",
      "[84]\ttrain-rmse:0.45180\tval-rmse:0.45811\n",
      "[85]\ttrain-rmse:0.45130\tval-rmse:0.45767\n",
      "[86]\ttrain-rmse:0.45079\tval-rmse:0.45722\n",
      "[87]\ttrain-rmse:0.45025\tval-rmse:0.45674\n",
      "[88]\ttrain-rmse:0.44978\tval-rmse:0.45633\n",
      "[89]\ttrain-rmse:0.44928\tval-rmse:0.45586\n",
      "[90]\ttrain-rmse:0.44876\tval-rmse:0.45539\n",
      "[91]\ttrain-rmse:0.44827\tval-rmse:0.45496\n",
      "[92]\ttrain-rmse:0.44774\tval-rmse:0.45453\n",
      "[93]\ttrain-rmse:0.44723\tval-rmse:0.45408\n",
      "[94]\ttrain-rmse:0.44670\tval-rmse:0.45364\n",
      "[95]\ttrain-rmse:0.44620\tval-rmse:0.45317\n",
      "[96]\ttrain-rmse:0.44572\tval-rmse:0.45275\n",
      "[97]\ttrain-rmse:0.44524\tval-rmse:0.45237\n",
      "[98]\ttrain-rmse:0.44475\tval-rmse:0.45192\n",
      "[99]\ttrain-rmse:0.44426\tval-rmse:0.45148\n",
      "[100]\ttrain-rmse:0.44377\tval-rmse:0.45108\n",
      "[101]\ttrain-rmse:0.44329\tval-rmse:0.45060\n",
      "[102]\ttrain-rmse:0.44284\tval-rmse:0.45021\n",
      "[103]\ttrain-rmse:0.44237\tval-rmse:0.44983\n",
      "[104]\ttrain-rmse:0.44188\tval-rmse:0.44941\n",
      "[105]\ttrain-rmse:0.44141\tval-rmse:0.44898\n",
      "[106]\ttrain-rmse:0.44096\tval-rmse:0.44860\n",
      "[107]\ttrain-rmse:0.44046\tval-rmse:0.44821\n",
      "[108]\ttrain-rmse:0.44001\tval-rmse:0.44780\n",
      "[109]\ttrain-rmse:0.43956\tval-rmse:0.44742\n",
      "[110]\ttrain-rmse:0.43906\tval-rmse:0.44703\n",
      "[111]\ttrain-rmse:0.43862\tval-rmse:0.44666\n",
      "[112]\ttrain-rmse:0.43814\tval-rmse:0.44619\n",
      "[113]\ttrain-rmse:0.43767\tval-rmse:0.44579\n",
      "[114]\ttrain-rmse:0.43717\tval-rmse:0.44532\n",
      "[115]\ttrain-rmse:0.43672\tval-rmse:0.44490\n",
      "[116]\ttrain-rmse:0.43623\tval-rmse:0.44450\n",
      "[117]\ttrain-rmse:0.43575\tval-rmse:0.44411\n",
      "[118]\ttrain-rmse:0.43530\tval-rmse:0.44370\n",
      "[119]\ttrain-rmse:0.43484\tval-rmse:0.44327\n",
      "[120]\ttrain-rmse:0.43439\tval-rmse:0.44285\n",
      "[121]\ttrain-rmse:0.43394\tval-rmse:0.44248\n",
      "[122]\ttrain-rmse:0.43349\tval-rmse:0.44212\n",
      "[123]\ttrain-rmse:0.43303\tval-rmse:0.44170\n",
      "[124]\ttrain-rmse:0.43258\tval-rmse:0.44132\n",
      "[125]\ttrain-rmse:0.43212\tval-rmse:0.44091\n",
      "[126]\ttrain-rmse:0.43166\tval-rmse:0.44049\n",
      "[127]\ttrain-rmse:0.43121\tval-rmse:0.44008\n",
      "[128]\ttrain-rmse:0.43073\tval-rmse:0.43972\n",
      "[129]\ttrain-rmse:0.43027\tval-rmse:0.43932\n",
      "[130]\ttrain-rmse:0.42980\tval-rmse:0.43894\n",
      "[131]\ttrain-rmse:0.42934\tval-rmse:0.43855\n",
      "[132]\ttrain-rmse:0.42886\tval-rmse:0.43817\n",
      "[133]\ttrain-rmse:0.42840\tval-rmse:0.43782\n",
      "[134]\ttrain-rmse:0.42795\tval-rmse:0.43743\n",
      "[135]\ttrain-rmse:0.42753\tval-rmse:0.43705\n",
      "[136]\ttrain-rmse:0.42708\tval-rmse:0.43666\n",
      "[137]\ttrain-rmse:0.42665\tval-rmse:0.43631\n",
      "[138]\ttrain-rmse:0.42621\tval-rmse:0.43592\n",
      "[139]\ttrain-rmse:0.42577\tval-rmse:0.43553\n",
      "[140]\ttrain-rmse:0.42536\tval-rmse:0.43516\n",
      "[141]\ttrain-rmse:0.42492\tval-rmse:0.43480\n",
      "[142]\ttrain-rmse:0.42450\tval-rmse:0.43447\n",
      "[143]\ttrain-rmse:0.42407\tval-rmse:0.43409\n",
      "[144]\ttrain-rmse:0.42363\tval-rmse:0.43369\n",
      "[145]\ttrain-rmse:0.42322\tval-rmse:0.43336\n",
      "[146]\ttrain-rmse:0.42278\tval-rmse:0.43298\n",
      "[147]\ttrain-rmse:0.42237\tval-rmse:0.43263\n",
      "[148]\ttrain-rmse:0.42193\tval-rmse:0.43225\n",
      "[149]\ttrain-rmse:0.42149\tval-rmse:0.43187\n",
      "[150]\ttrain-rmse:0.42107\tval-rmse:0.43154\n",
      "[151]\ttrain-rmse:0.42062\tval-rmse:0.43116\n",
      "[152]\ttrain-rmse:0.42019\tval-rmse:0.43078\n",
      "[153]\ttrain-rmse:0.41976\tval-rmse:0.43040\n",
      "[154]\ttrain-rmse:0.41935\tval-rmse:0.43003\n",
      "[155]\ttrain-rmse:0.41891\tval-rmse:0.42968\n",
      "[156]\ttrain-rmse:0.41848\tval-rmse:0.42931\n",
      "[157]\ttrain-rmse:0.41807\tval-rmse:0.42895\n",
      "[158]\ttrain-rmse:0.41764\tval-rmse:0.42858\n",
      "[159]\ttrain-rmse:0.41724\tval-rmse:0.42823\n",
      "[160]\ttrain-rmse:0.41684\tval-rmse:0.42786\n",
      "[161]\ttrain-rmse:0.41643\tval-rmse:0.42751\n",
      "[162]\ttrain-rmse:0.41599\tval-rmse:0.42715\n",
      "[163]\ttrain-rmse:0.41558\tval-rmse:0.42677\n",
      "[164]\ttrain-rmse:0.41513\tval-rmse:0.42636\n",
      "[165]\ttrain-rmse:0.41470\tval-rmse:0.42602\n",
      "[166]\ttrain-rmse:0.41428\tval-rmse:0.42568\n",
      "[167]\ttrain-rmse:0.41385\tval-rmse:0.42532\n",
      "[168]\ttrain-rmse:0.41343\tval-rmse:0.42493\n",
      "[169]\ttrain-rmse:0.41301\tval-rmse:0.42456\n",
      "[170]\ttrain-rmse:0.41261\tval-rmse:0.42419\n",
      "[171]\ttrain-rmse:0.41218\tval-rmse:0.42383\n",
      "[172]\ttrain-rmse:0.41177\tval-rmse:0.42347\n",
      "[173]\ttrain-rmse:0.41133\tval-rmse:0.42315\n",
      "[174]\ttrain-rmse:0.41092\tval-rmse:0.42279\n",
      "[175]\ttrain-rmse:0.41049\tval-rmse:0.42245\n",
      "[176]\ttrain-rmse:0.41005\tval-rmse:0.42209\n",
      "[177]\ttrain-rmse:0.40964\tval-rmse:0.42175\n",
      "[178]\ttrain-rmse:0.40924\tval-rmse:0.42143\n",
      "[179]\ttrain-rmse:0.40886\tval-rmse:0.42109\n",
      "[180]\ttrain-rmse:0.40843\tval-rmse:0.42070\n",
      "[181]\ttrain-rmse:0.40801\tval-rmse:0.42035\n",
      "[182]\ttrain-rmse:0.40762\tval-rmse:0.41999\n",
      "[183]\ttrain-rmse:0.40723\tval-rmse:0.41965\n",
      "[184]\ttrain-rmse:0.40685\tval-rmse:0.41934\n",
      "[185]\ttrain-rmse:0.40645\tval-rmse:0.41903\n",
      "[186]\ttrain-rmse:0.40608\tval-rmse:0.41869\n",
      "[187]\ttrain-rmse:0.40564\tval-rmse:0.41832\n",
      "[188]\ttrain-rmse:0.40525\tval-rmse:0.41799\n",
      "[189]\ttrain-rmse:0.40486\tval-rmse:0.41763\n",
      "[190]\ttrain-rmse:0.40445\tval-rmse:0.41726\n",
      "[191]\ttrain-rmse:0.40405\tval-rmse:0.41692\n",
      "[192]\ttrain-rmse:0.40363\tval-rmse:0.41656\n",
      "[193]\ttrain-rmse:0.40322\tval-rmse:0.41621\n",
      "[194]\ttrain-rmse:0.40280\tval-rmse:0.41585\n",
      "[195]\ttrain-rmse:0.40242\tval-rmse:0.41551\n",
      "[196]\ttrain-rmse:0.40206\tval-rmse:0.41519\n",
      "[197]\ttrain-rmse:0.40168\tval-rmse:0.41487\n",
      "[198]\ttrain-rmse:0.40130\tval-rmse:0.41457\n",
      "[199]\ttrain-rmse:0.40090\tval-rmse:0.41422\n",
      "[200]\ttrain-rmse:0.40051\tval-rmse:0.41387\n",
      "[201]\ttrain-rmse:0.40011\tval-rmse:0.41352\n",
      "[202]\ttrain-rmse:0.39976\tval-rmse:0.41321\n",
      "[203]\ttrain-rmse:0.39938\tval-rmse:0.41287\n",
      "[204]\ttrain-rmse:0.39903\tval-rmse:0.41259\n",
      "[205]\ttrain-rmse:0.39868\tval-rmse:0.41230\n",
      "[206]\ttrain-rmse:0.39829\tval-rmse:0.41196\n",
      "[207]\ttrain-rmse:0.39791\tval-rmse:0.41162\n",
      "[208]\ttrain-rmse:0.39751\tval-rmse:0.41129\n",
      "[209]\ttrain-rmse:0.39712\tval-rmse:0.41095\n",
      "[210]\ttrain-rmse:0.39675\tval-rmse:0.41063\n",
      "[211]\ttrain-rmse:0.39636\tval-rmse:0.41031\n",
      "[212]\ttrain-rmse:0.39600\tval-rmse:0.41000\n",
      "[213]\ttrain-rmse:0.39563\tval-rmse:0.40965\n",
      "[214]\ttrain-rmse:0.39528\tval-rmse:0.40934\n",
      "[215]\ttrain-rmse:0.39492\tval-rmse:0.40904\n",
      "[216]\ttrain-rmse:0.39454\tval-rmse:0.40871\n",
      "[217]\ttrain-rmse:0.39417\tval-rmse:0.40840\n",
      "[218]\ttrain-rmse:0.39380\tval-rmse:0.40808\n",
      "[219]\ttrain-rmse:0.39343\tval-rmse:0.40778\n",
      "[220]\ttrain-rmse:0.39308\tval-rmse:0.40748\n",
      "[221]\ttrain-rmse:0.39270\tval-rmse:0.40715\n",
      "[222]\ttrain-rmse:0.39231\tval-rmse:0.40678\n",
      "[223]\ttrain-rmse:0.39195\tval-rmse:0.40645\n",
      "[224]\ttrain-rmse:0.39157\tval-rmse:0.40610\n",
      "[225]\ttrain-rmse:0.39119\tval-rmse:0.40582\n",
      "[226]\ttrain-rmse:0.39081\tval-rmse:0.40551\n",
      "[227]\ttrain-rmse:0.39045\tval-rmse:0.40523\n",
      "[228]\ttrain-rmse:0.39010\tval-rmse:0.40490\n",
      "[229]\ttrain-rmse:0.38973\tval-rmse:0.40460\n",
      "[230]\ttrain-rmse:0.38936\tval-rmse:0.40430\n",
      "[231]\ttrain-rmse:0.38899\tval-rmse:0.40401\n",
      "[232]\ttrain-rmse:0.38861\tval-rmse:0.40366\n",
      "[233]\ttrain-rmse:0.38828\tval-rmse:0.40335\n",
      "[234]\ttrain-rmse:0.38793\tval-rmse:0.40303\n",
      "[235]\ttrain-rmse:0.38758\tval-rmse:0.40276\n",
      "[236]\ttrain-rmse:0.38722\tval-rmse:0.40245\n",
      "[237]\ttrain-rmse:0.38687\tval-rmse:0.40214\n",
      "[238]\ttrain-rmse:0.38650\tval-rmse:0.40188\n",
      "[239]\ttrain-rmse:0.38612\tval-rmse:0.40155\n",
      "[240]\ttrain-rmse:0.38577\tval-rmse:0.40123\n",
      "[241]\ttrain-rmse:0.38544\tval-rmse:0.40096\n",
      "[242]\ttrain-rmse:0.38509\tval-rmse:0.40065\n",
      "[243]\ttrain-rmse:0.38472\tval-rmse:0.40034\n",
      "[244]\ttrain-rmse:0.38435\tval-rmse:0.40006\n",
      "[245]\ttrain-rmse:0.38399\tval-rmse:0.39977\n",
      "[246]\ttrain-rmse:0.38362\tval-rmse:0.39946\n",
      "[247]\ttrain-rmse:0.38327\tval-rmse:0.39916\n",
      "[248]\ttrain-rmse:0.38293\tval-rmse:0.39886\n",
      "[249]\ttrain-rmse:0.38260\tval-rmse:0.39857\n",
      "[250]\ttrain-rmse:0.38228\tval-rmse:0.39828\n",
      "[251]\ttrain-rmse:0.38194\tval-rmse:0.39795\n",
      "[252]\ttrain-rmse:0.38161\tval-rmse:0.39767\n",
      "[253]\ttrain-rmse:0.38126\tval-rmse:0.39741\n",
      "[254]\ttrain-rmse:0.38091\tval-rmse:0.39714\n",
      "[255]\ttrain-rmse:0.38057\tval-rmse:0.39688\n",
      "[256]\ttrain-rmse:0.38023\tval-rmse:0.39659\n",
      "[257]\ttrain-rmse:0.37992\tval-rmse:0.39635\n",
      "[258]\ttrain-rmse:0.37958\tval-rmse:0.39605\n",
      "[259]\ttrain-rmse:0.37923\tval-rmse:0.39575\n",
      "[260]\ttrain-rmse:0.37890\tval-rmse:0.39551\n",
      "[261]\ttrain-rmse:0.37854\tval-rmse:0.39522\n",
      "[262]\ttrain-rmse:0.37820\tval-rmse:0.39494\n",
      "[263]\ttrain-rmse:0.37786\tval-rmse:0.39467\n",
      "[264]\ttrain-rmse:0.37752\tval-rmse:0.39442\n",
      "[265]\ttrain-rmse:0.37721\tval-rmse:0.39417\n",
      "[266]\ttrain-rmse:0.37687\tval-rmse:0.39388\n",
      "[267]\ttrain-rmse:0.37654\tval-rmse:0.39361\n",
      "[268]\ttrain-rmse:0.37622\tval-rmse:0.39335\n",
      "[269]\ttrain-rmse:0.37587\tval-rmse:0.39307\n",
      "[270]\ttrain-rmse:0.37553\tval-rmse:0.39277\n",
      "[271]\ttrain-rmse:0.37520\tval-rmse:0.39251\n",
      "[272]\ttrain-rmse:0.37489\tval-rmse:0.39223\n",
      "[273]\ttrain-rmse:0.37460\tval-rmse:0.39198\n",
      "[274]\ttrain-rmse:0.37427\tval-rmse:0.39167\n",
      "[275]\ttrain-rmse:0.37394\tval-rmse:0.39140\n",
      "[276]\ttrain-rmse:0.37362\tval-rmse:0.39113\n",
      "[277]\ttrain-rmse:0.37329\tval-rmse:0.39085\n",
      "[278]\ttrain-rmse:0.37296\tval-rmse:0.39056\n",
      "[279]\ttrain-rmse:0.37263\tval-rmse:0.39028\n",
      "[280]\ttrain-rmse:0.37228\tval-rmse:0.38998\n",
      "[281]\ttrain-rmse:0.37196\tval-rmse:0.38968\n",
      "[282]\ttrain-rmse:0.37163\tval-rmse:0.38939\n",
      "[283]\ttrain-rmse:0.37130\tval-rmse:0.38912\n",
      "[284]\ttrain-rmse:0.37098\tval-rmse:0.38882\n",
      "[285]\ttrain-rmse:0.37065\tval-rmse:0.38857\n",
      "[286]\ttrain-rmse:0.37033\tval-rmse:0.38831\n",
      "[287]\ttrain-rmse:0.37000\tval-rmse:0.38801\n",
      "[288]\ttrain-rmse:0.36967\tval-rmse:0.38776\n",
      "[289]\ttrain-rmse:0.36934\tval-rmse:0.38750\n",
      "[290]\ttrain-rmse:0.36903\tval-rmse:0.38725\n",
      "[291]\ttrain-rmse:0.36873\tval-rmse:0.38700\n",
      "[292]\ttrain-rmse:0.36842\tval-rmse:0.38675\n",
      "[293]\ttrain-rmse:0.36814\tval-rmse:0.38651\n",
      "[294]\ttrain-rmse:0.36781\tval-rmse:0.38626\n",
      "[295]\ttrain-rmse:0.36748\tval-rmse:0.38599\n",
      "[296]\ttrain-rmse:0.36716\tval-rmse:0.38570\n",
      "[297]\ttrain-rmse:0.36686\tval-rmse:0.38543\n",
      "[298]\ttrain-rmse:0.36657\tval-rmse:0.38518\n",
      "[299]\ttrain-rmse:0.36626\tval-rmse:0.38492\n",
      "[300]\ttrain-rmse:0.36596\tval-rmse:0.38466\n",
      "[301]\ttrain-rmse:0.36564\tval-rmse:0.38441\n",
      "[302]\ttrain-rmse:0.36530\tval-rmse:0.38416\n",
      "[303]\ttrain-rmse:0.36498\tval-rmse:0.38391\n",
      "[304]\ttrain-rmse:0.36470\tval-rmse:0.38367\n",
      "[305]\ttrain-rmse:0.36443\tval-rmse:0.38341\n",
      "[306]\ttrain-rmse:0.36410\tval-rmse:0.38315\n",
      "[307]\ttrain-rmse:0.36380\tval-rmse:0.38288\n",
      "[308]\ttrain-rmse:0.36350\tval-rmse:0.38264\n",
      "[309]\ttrain-rmse:0.36320\tval-rmse:0.38237\n",
      "[310]\ttrain-rmse:0.36287\tval-rmse:0.38210\n",
      "[311]\ttrain-rmse:0.36260\tval-rmse:0.38187\n",
      "[312]\ttrain-rmse:0.36230\tval-rmse:0.38162\n",
      "[313]\ttrain-rmse:0.36200\tval-rmse:0.38140\n",
      "[314]\ttrain-rmse:0.36170\tval-rmse:0.38116\n",
      "[315]\ttrain-rmse:0.36141\tval-rmse:0.38091\n",
      "[316]\ttrain-rmse:0.36110\tval-rmse:0.38067\n",
      "[317]\ttrain-rmse:0.36081\tval-rmse:0.38042\n",
      "[318]\ttrain-rmse:0.36052\tval-rmse:0.38019\n",
      "[319]\ttrain-rmse:0.36023\tval-rmse:0.37997\n",
      "[320]\ttrain-rmse:0.35993\tval-rmse:0.37974\n",
      "[321]\ttrain-rmse:0.35964\tval-rmse:0.37949\n",
      "[322]\ttrain-rmse:0.35935\tval-rmse:0.37926\n",
      "[323]\ttrain-rmse:0.35906\tval-rmse:0.37900\n",
      "[324]\ttrain-rmse:0.35875\tval-rmse:0.37878\n",
      "[325]\ttrain-rmse:0.35841\tval-rmse:0.37850\n",
      "[326]\ttrain-rmse:0.35813\tval-rmse:0.37826\n",
      "[327]\ttrain-rmse:0.35785\tval-rmse:0.37798\n",
      "[328]\ttrain-rmse:0.35755\tval-rmse:0.37774\n",
      "[329]\ttrain-rmse:0.35727\tval-rmse:0.37749\n",
      "[330]\ttrain-rmse:0.35697\tval-rmse:0.37725\n",
      "[331]\ttrain-rmse:0.35669\tval-rmse:0.37701\n",
      "[332]\ttrain-rmse:0.35639\tval-rmse:0.37676\n",
      "[333]\ttrain-rmse:0.35611\tval-rmse:0.37650\n",
      "[334]\ttrain-rmse:0.35581\tval-rmse:0.37628\n",
      "[335]\ttrain-rmse:0.35553\tval-rmse:0.37606\n",
      "[336]\ttrain-rmse:0.35522\tval-rmse:0.37581\n",
      "[337]\ttrain-rmse:0.35495\tval-rmse:0.37560\n",
      "[338]\ttrain-rmse:0.35466\tval-rmse:0.37535\n",
      "[339]\ttrain-rmse:0.35438\tval-rmse:0.37515\n",
      "[340]\ttrain-rmse:0.35411\tval-rmse:0.37493\n",
      "[341]\ttrain-rmse:0.35384\tval-rmse:0.37469\n",
      "[342]\ttrain-rmse:0.35356\tval-rmse:0.37448\n",
      "[343]\ttrain-rmse:0.35327\tval-rmse:0.37423\n",
      "[344]\ttrain-rmse:0.35298\tval-rmse:0.37400\n",
      "[345]\ttrain-rmse:0.35271\tval-rmse:0.37379\n",
      "[346]\ttrain-rmse:0.35242\tval-rmse:0.37355\n",
      "[347]\ttrain-rmse:0.35214\tval-rmse:0.37332\n",
      "[348]\ttrain-rmse:0.35187\tval-rmse:0.37309\n",
      "[349]\ttrain-rmse:0.35158\tval-rmse:0.37283\n",
      "[350]\ttrain-rmse:0.35128\tval-rmse:0.37261\n",
      "[351]\ttrain-rmse:0.35102\tval-rmse:0.37239\n",
      "[352]\ttrain-rmse:0.35075\tval-rmse:0.37219\n",
      "[353]\ttrain-rmse:0.35049\tval-rmse:0.37198\n",
      "[354]\ttrain-rmse:0.35023\tval-rmse:0.37176\n",
      "[355]\ttrain-rmse:0.34997\tval-rmse:0.37151\n",
      "[356]\ttrain-rmse:0.34969\tval-rmse:0.37131\n",
      "[357]\ttrain-rmse:0.34939\tval-rmse:0.37107\n",
      "[358]\ttrain-rmse:0.34912\tval-rmse:0.37085\n",
      "[359]\ttrain-rmse:0.34884\tval-rmse:0.37062\n",
      "[360]\ttrain-rmse:0.34858\tval-rmse:0.37041\n",
      "[361]\ttrain-rmse:0.34829\tval-rmse:0.37019\n",
      "[362]\ttrain-rmse:0.34802\tval-rmse:0.36995\n",
      "[363]\ttrain-rmse:0.34776\tval-rmse:0.36977\n",
      "[364]\ttrain-rmse:0.34750\tval-rmse:0.36953\n",
      "[365]\ttrain-rmse:0.34722\tval-rmse:0.36931\n",
      "[366]\ttrain-rmse:0.34695\tval-rmse:0.36908\n",
      "[367]\ttrain-rmse:0.34666\tval-rmse:0.36887\n",
      "[368]\ttrain-rmse:0.34641\tval-rmse:0.36868\n",
      "[369]\ttrain-rmse:0.34613\tval-rmse:0.36846\n",
      "[370]\ttrain-rmse:0.34586\tval-rmse:0.36826\n",
      "[371]\ttrain-rmse:0.34557\tval-rmse:0.36805\n",
      "[372]\ttrain-rmse:0.34531\tval-rmse:0.36786\n",
      "[373]\ttrain-rmse:0.34506\tval-rmse:0.36764\n",
      "[374]\ttrain-rmse:0.34480\tval-rmse:0.36743\n",
      "[375]\ttrain-rmse:0.34452\tval-rmse:0.36720\n",
      "[376]\ttrain-rmse:0.34425\tval-rmse:0.36697\n",
      "[377]\ttrain-rmse:0.34400\tval-rmse:0.36678\n",
      "[378]\ttrain-rmse:0.34371\tval-rmse:0.36654\n",
      "[379]\ttrain-rmse:0.34345\tval-rmse:0.36630\n",
      "[380]\ttrain-rmse:0.34319\tval-rmse:0.36609\n",
      "[381]\ttrain-rmse:0.34294\tval-rmse:0.36587\n",
      "[382]\ttrain-rmse:0.34268\tval-rmse:0.36563\n",
      "[383]\ttrain-rmse:0.34243\tval-rmse:0.36543\n",
      "[384]\ttrain-rmse:0.34218\tval-rmse:0.36521\n",
      "[385]\ttrain-rmse:0.34191\tval-rmse:0.36499\n",
      "[386]\ttrain-rmse:0.34165\tval-rmse:0.36479\n",
      "[387]\ttrain-rmse:0.34139\tval-rmse:0.36457\n",
      "[388]\ttrain-rmse:0.34112\tval-rmse:0.36438\n",
      "[389]\ttrain-rmse:0.34085\tval-rmse:0.36417\n",
      "[390]\ttrain-rmse:0.34060\tval-rmse:0.36399\n",
      "[391]\ttrain-rmse:0.34034\tval-rmse:0.36377\n",
      "[392]\ttrain-rmse:0.34007\tval-rmse:0.36358\n",
      "[393]\ttrain-rmse:0.33984\tval-rmse:0.36337\n",
      "[394]\ttrain-rmse:0.33959\tval-rmse:0.36317\n",
      "[395]\ttrain-rmse:0.33932\tval-rmse:0.36298\n",
      "[396]\ttrain-rmse:0.33906\tval-rmse:0.36277\n",
      "[397]\ttrain-rmse:0.33881\tval-rmse:0.36257\n",
      "[398]\ttrain-rmse:0.33855\tval-rmse:0.36238\n",
      "[399]\ttrain-rmse:0.33832\tval-rmse:0.36219\n",
      "[400]\ttrain-rmse:0.33806\tval-rmse:0.36199\n",
      "[401]\ttrain-rmse:0.33779\tval-rmse:0.36178\n",
      "[402]\ttrain-rmse:0.33757\tval-rmse:0.36159\n",
      "[403]\ttrain-rmse:0.33732\tval-rmse:0.36140\n",
      "[404]\ttrain-rmse:0.33707\tval-rmse:0.36118\n",
      "[405]\ttrain-rmse:0.33683\tval-rmse:0.36096\n",
      "[406]\ttrain-rmse:0.33660\tval-rmse:0.36076\n",
      "[407]\ttrain-rmse:0.33634\tval-rmse:0.36057\n",
      "[408]\ttrain-rmse:0.33609\tval-rmse:0.36038\n",
      "[409]\ttrain-rmse:0.33585\tval-rmse:0.36019\n",
      "[410]\ttrain-rmse:0.33560\tval-rmse:0.35998\n",
      "[411]\ttrain-rmse:0.33535\tval-rmse:0.35977\n",
      "[412]\ttrain-rmse:0.33510\tval-rmse:0.35957\n",
      "[413]\ttrain-rmse:0.33485\tval-rmse:0.35938\n",
      "[414]\ttrain-rmse:0.33459\tval-rmse:0.35919\n",
      "[415]\ttrain-rmse:0.33436\tval-rmse:0.35903\n",
      "[416]\ttrain-rmse:0.33411\tval-rmse:0.35885\n",
      "[417]\ttrain-rmse:0.33385\tval-rmse:0.35866\n",
      "[418]\ttrain-rmse:0.33363\tval-rmse:0.35846\n",
      "[419]\ttrain-rmse:0.33342\tval-rmse:0.35825\n",
      "[420]\ttrain-rmse:0.33320\tval-rmse:0.35808\n",
      "[421]\ttrain-rmse:0.33298\tval-rmse:0.35789\n",
      "[422]\ttrain-rmse:0.33272\tval-rmse:0.35771\n",
      "[423]\ttrain-rmse:0.33248\tval-rmse:0.35753\n",
      "[424]\ttrain-rmse:0.33222\tval-rmse:0.35735\n",
      "[425]\ttrain-rmse:0.33198\tval-rmse:0.35715\n",
      "[426]\ttrain-rmse:0.33177\tval-rmse:0.35698\n",
      "[427]\ttrain-rmse:0.33152\tval-rmse:0.35681\n",
      "[428]\ttrain-rmse:0.33128\tval-rmse:0.35659\n",
      "[429]\ttrain-rmse:0.33105\tval-rmse:0.35640\n",
      "[430]\ttrain-rmse:0.33081\tval-rmse:0.35623\n",
      "[431]\ttrain-rmse:0.33059\tval-rmse:0.35607\n",
      "[432]\ttrain-rmse:0.33037\tval-rmse:0.35589\n",
      "[433]\ttrain-rmse:0.33013\tval-rmse:0.35568\n",
      "[434]\ttrain-rmse:0.32989\tval-rmse:0.35550\n",
      "[435]\ttrain-rmse:0.32968\tval-rmse:0.35533\n",
      "[436]\ttrain-rmse:0.32945\tval-rmse:0.35515\n",
      "[437]\ttrain-rmse:0.32920\tval-rmse:0.35497\n",
      "[438]\ttrain-rmse:0.32897\tval-rmse:0.35479\n",
      "[439]\ttrain-rmse:0.32874\tval-rmse:0.35463\n",
      "[440]\ttrain-rmse:0.32850\tval-rmse:0.35447\n",
      "[441]\ttrain-rmse:0.32827\tval-rmse:0.35430\n",
      "[442]\ttrain-rmse:0.32805\tval-rmse:0.35414\n",
      "[443]\ttrain-rmse:0.32781\tval-rmse:0.35397\n",
      "[444]\ttrain-rmse:0.32759\tval-rmse:0.35382\n",
      "[445]\ttrain-rmse:0.32738\tval-rmse:0.35366\n",
      "[446]\ttrain-rmse:0.32714\tval-rmse:0.35348\n",
      "[447]\ttrain-rmse:0.32693\tval-rmse:0.35331\n",
      "[448]\ttrain-rmse:0.32671\tval-rmse:0.35312\n",
      "[449]\ttrain-rmse:0.32647\tval-rmse:0.35295\n",
      "[450]\ttrain-rmse:0.32624\tval-rmse:0.35276\n",
      "[451]\ttrain-rmse:0.32602\tval-rmse:0.35262\n",
      "[452]\ttrain-rmse:0.32579\tval-rmse:0.35246\n",
      "[453]\ttrain-rmse:0.32557\tval-rmse:0.35227\n",
      "[454]\ttrain-rmse:0.32533\tval-rmse:0.35207\n",
      "[455]\ttrain-rmse:0.32511\tval-rmse:0.35191\n",
      "[456]\ttrain-rmse:0.32488\tval-rmse:0.35173\n",
      "[457]\ttrain-rmse:0.32468\tval-rmse:0.35156\n",
      "[458]\ttrain-rmse:0.32447\tval-rmse:0.35141\n",
      "[459]\ttrain-rmse:0.32424\tval-rmse:0.35124\n",
      "[460]\ttrain-rmse:0.32400\tval-rmse:0.35104\n",
      "[461]\ttrain-rmse:0.32376\tval-rmse:0.35087\n",
      "[462]\ttrain-rmse:0.32357\tval-rmse:0.35071\n",
      "[463]\ttrain-rmse:0.32336\tval-rmse:0.35054\n",
      "[464]\ttrain-rmse:0.32312\tval-rmse:0.35036\n",
      "[465]\ttrain-rmse:0.32293\tval-rmse:0.35021\n",
      "[466]\ttrain-rmse:0.32273\tval-rmse:0.35006\n",
      "[467]\ttrain-rmse:0.32251\tval-rmse:0.34992\n",
      "[468]\ttrain-rmse:0.32227\tval-rmse:0.34974\n",
      "[469]\ttrain-rmse:0.32204\tval-rmse:0.34956\n",
      "[470]\ttrain-rmse:0.32182\tval-rmse:0.34939\n",
      "[471]\ttrain-rmse:0.32163\tval-rmse:0.34923\n",
      "[472]\ttrain-rmse:0.32139\tval-rmse:0.34906\n",
      "[473]\ttrain-rmse:0.32118\tval-rmse:0.34892\n",
      "[474]\ttrain-rmse:0.32095\tval-rmse:0.34875\n",
      "[475]\ttrain-rmse:0.32076\tval-rmse:0.34861\n",
      "[476]\ttrain-rmse:0.32055\tval-rmse:0.34845\n",
      "[477]\ttrain-rmse:0.32032\tval-rmse:0.34829\n",
      "[478]\ttrain-rmse:0.32011\tval-rmse:0.34815\n",
      "[479]\ttrain-rmse:0.31990\tval-rmse:0.34798\n",
      "[480]\ttrain-rmse:0.31966\tval-rmse:0.34779\n",
      "[481]\ttrain-rmse:0.31944\tval-rmse:0.34763\n",
      "[482]\ttrain-rmse:0.31923\tval-rmse:0.34748\n",
      "[483]\ttrain-rmse:0.31902\tval-rmse:0.34735\n",
      "[484]\ttrain-rmse:0.31881\tval-rmse:0.34716\n",
      "[485]\ttrain-rmse:0.31859\tval-rmse:0.34700\n",
      "[486]\ttrain-rmse:0.31839\tval-rmse:0.34687\n",
      "[487]\ttrain-rmse:0.31817\tval-rmse:0.34670\n",
      "[488]\ttrain-rmse:0.31796\tval-rmse:0.34654\n",
      "[489]\ttrain-rmse:0.31775\tval-rmse:0.34638\n",
      "[490]\ttrain-rmse:0.31754\tval-rmse:0.34623\n",
      "[491]\ttrain-rmse:0.31732\tval-rmse:0.34608\n",
      "[492]\ttrain-rmse:0.31710\tval-rmse:0.34592\n",
      "[493]\ttrain-rmse:0.31690\tval-rmse:0.34577\n",
      "[494]\ttrain-rmse:0.31671\tval-rmse:0.34562\n",
      "[495]\ttrain-rmse:0.31648\tval-rmse:0.34547\n",
      "[496]\ttrain-rmse:0.31628\tval-rmse:0.34533\n",
      "[497]\ttrain-rmse:0.31607\tval-rmse:0.34518\n",
      "[498]\ttrain-rmse:0.31588\tval-rmse:0.34504\n",
      "[499]\ttrain-rmse:0.31567\tval-rmse:0.34489\n",
      "best iteration 499\n",
      "Accuracy: 90.90%\n",
      "Error Rate: 9.10%\n",
      "xgboost success! \n",
      " cost time: 10.657466173171997 (s)......\n"
     ]
    }
   ],
   "source": [
    "#训练模型并保存  \n",
    "# early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练  \n",
    "model = xgb.train(  \n",
    "    plst,  \n",
    "    xgb_train,  \n",
    "    num_rounds,  \n",
    "    watchlist,  \n",
    "    early_stopping_rounds=100,  \n",
    ")  \n",
    "#model.save_model('./model/xgb.model') # 用于存储训练出的模型  \n",
    "print(\"best iteration\", model.best_iteration)  \n",
    "  \n",
    "y_pred = model.predict(xgb_test)  \n",
    "  \n",
    "# 计算预测准确率  \n",
    "predicted_correctly = sum(1 for i in range(len(y_pred)) if int(y_pred[i] > 0.5) == y_test[i])  \n",
    "accuracy = predicted_correctly / float(len(y_pred))  \n",
    "print('Accuracy: %.2f%%' % (accuracy * 100.0))  \n",
    "  \n",
    "# 计算误差率  \n",
    "error_rate = sum(1 for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) / float(len(y_pred))  \n",
    "print('Error Rate: %.2f%%' % (error_rate * 100.0))  \n",
    "  \n",
    "# 输出运行时长  \n",
    "cost_time = time.time() - start_time  \n",
    "print(\"xgboost success!\", '\\n', \"cost time:\", cost_time, \"(s)......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    #     silent=0,  #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "    #nthread=4,# cpu 线程数 默认最大\n",
    "    learning_rate=0.3,  # 如同学习率\n",
    "    min_child_weight=1,\n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    max_depth=6,  # 构建树的深度，越大越容易过拟合\n",
    "    gamma=0,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    subsample=1,  # 随机采样训练样本 训练实例的子采样比\n",
    "    max_delta_step=0,  #最大增量步长，我们允许每个树的权重估计。\n",
    "    colsample_bytree=1,  # 生成树时进行的列采样 \n",
    "    reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    #reg_alpha=0, # L1 正则项参数\n",
    "    #scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "    #objective= 'multi:softmax', #多分类的问题 指定学习任务和相应的学习目标\n",
    "    #num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "    n_estimators=100,  #树的个数\n",
    "    seed=1000  #随机种子\n",
    "    #eval_metric= 'auc'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.488111\n",
      "Save model...\n",
      "Start predicting...\n",
      "error=0.081333\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 加载你的数据\n",
    "# print('Load data...')\n",
    "# df_train = pd.read_csv('../regression/regression.train', header=None, sep='\\t')\n",
    "# df_test = pd.read_csv('../regression/regression.test', header=None, sep='\\t')\n",
    "#\n",
    "# y_train = df_train[0].values\n",
    "# y_test = df_test[0].values\n",
    "# X_train = df_train.drop(0, axis=1).values\n",
    "# X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "# 创建成lgb特征的数据集格式\n",
    "lgb_train = lgb.Dataset(X_train, y_train)  # 将数据保存到LightGBM二进制文件将使加载更快\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)  # 创建验证数据\n",
    "\n",
    "# 将参数写成字典下形式\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  # 设置提升类型\n",
    "    'objective': 'regression',  # 目标函数\n",
    "    'metric': {'l2', 'auc'},  # 评估函数\n",
    "    'num_leaves': 31,  # 叶子节点数\n",
    "    'learning_rate': 0.05,  # 学习速率\n",
    "    'feature_fraction': 0.9,  # 建树的特征选择比例\n",
    "    'bagging_fraction': 0.8,  # 建树的样本采样比例\n",
    "    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n",
    "    'verbose': 1  # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# 训练 cv and train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=lgb_eval)  # 训练数据需要参数列表和数据集\n",
    "\n",
    "print('Save model...')\n",
    "\n",
    "gbm.save_model('model.txt')  # 训练后保存模型到文件\n",
    "\n",
    "print('Start predicting...')\n",
    "# 预测数据集\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration\n",
    "                     )  #如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测\n",
    "# 评估模型\n",
    "print('error=%f' %\n",
    "      (sum(1\n",
    "           for i in range(len(y_pred)) if int(y_pred[i] > 0.5) != y_test[i]) /\n",
    "       float(len(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Number of positive: 4393, number of negative: 4607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488111 -> initscore=-0.047565\n",
      "[LightGBM] [Info] Start training from score -0.047565\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Number of positive: 4393, number of negative: 4607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 9000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488111 -> initscore=-0.047565\n",
      "[LightGBM] [Info] Start training from score -0.047565\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Accuracy : 0.9377\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    boosting_type='gbdt',  # 提升树的类型 gbdt,dart,goss,rf\n",
    "    num_leaves=31,  #树的最大叶子数，对比xgboost一般为2^(max_depth)\n",
    "    max_depth=-1,  #最大树的深度\n",
    "    learning_rate=0.1,  #学习率\n",
    "    n_estimators=100,  # 拟合的树的棵树，相当于训练轮数\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,  # 最小分割增益\n",
    "    min_child_weight=0.001,  # 分支结点的最小权重\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,  # 训练样本采样率 行\n",
    "    subsample_freq=0,  # 子样本频率\n",
    "    colsample_bytree=1.0,  # 训练特征采样率 列\n",
    "    reg_alpha=0.0,  # L1正则化系数\n",
    "    reg_lambda=0.0,  # L2正则化系数\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "#设置验证集合 verbose=False不打印过程\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
